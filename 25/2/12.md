# 25/02/12 TIL
## 경사하강법 (Gredient Descent)
  경사하강법은 최적화 알고리즘 중 하나로, 손실 함수(Loss Function)의 값을 최소화하는 파라미터(가중치(weight)와 편향(bias))를 구하기 위해 손실 함수에 대한 각 매개변수의 기울기를 이용하는 방법. 이 때 기울기가 0이 되는 지점이 손실함수가 가장 작은 지점으로 그 지점에 도달하기 위해 기울기의 반대 방향으로 움직여 찾는다.
### 배치 경사하강법 (Batch Gradient Descent, BGD)
  - 배치 경사하강법은 보통 경사하강법으로 얘기하며, 각 반복마다 모든 훈련 데이터 세트를 처리하는 방법이다. 즉 한 번의 반복에서 전체 데이터 샘플의 기울기를 계산한다. 기울기 계산 후 모든 매개변수에 대한 업데이트를 진행한 후 다시 최적의 해에 도달할 때까지 모든 데이터를 가지고 같은 계산을 반복하기에 수렴하는데 오랜 시간이 걸린다.

  - 장점:
    - 하나의 데이터 값이 아닌 모든 훈련 데이터 세트의 평균을 계산하여 매개변수를 업데이트하기 때문에 손실 함수의 전역 최솟값을 향한 노이즈가 별로 없다.
    - 벡터화의 이점을 누릴 수 있다.
  - 단점:
    - Local Otimal에 빠질 수 있으나 잡음이 별로 없어 Local optimal에서 벗어나기가 힘들다.
    - 한 번의 학습에 모든 데이터 세트를 사용하니 학습이 오래 걸린다.

### 확률적 경사하강법 (Stochastic Gradient Descent, SGD)
  - 확률적 경사하강법은 첫 단계로 일단 전체 훈련 데이터 세트를 무작위로 하나의 반복 안에서 하나의 데이터에 대해서만 손실함수에 대한 기울기를 계산하여 업데이트를 진행하는 방법이다. 첫 매개변수를 업데이트하면 그 다음 매개변수를, 또 그 다음인 세 번째부터 마지막 데이터까지 기울기 계산 후 업데이트한다. 물론 BGD와 마찬가지로 최적의 매개변수 값에 도달할 때까지 해당 계산을 반복한다.
  - 장점:
    - 한 번의 학습에 하나의 데이터만 처리되므로 계산 속도가 빠르다.
    - 데이터 세트가 더 크다면, 그만큼 매개변수를 더 자주 업데이트하게 되므로 더 빠르게 수렴할 수 있다.
    - 빈번한 업데이트로 생긴 잡음 덕에 local optimal에서 벗어날 수 있다.
  - 단점:
    - 빈번한 업데이트로 인해 비교적 노이즈가 심하다.
    - 노이즈가 많아 손실함수의 최솟값으로 수렴하는데 더 오래 걸릴 수 있다.
    - 대부분의 경우 전역 최솟값으로 가게 되지만 간혹 잘못된 방향으로 가기도 한다. 즉, 최적값을 찾지 못할 가능성도 있다는 것이다.
    - 한 번에 하나의 데이터만 다루니 벡터화의 이점을 잃는다.

### 미니배치 경사하강법 (Mini Batch Gradient Descent, MGD)
  BGD와 SGD보다 더 빠를게 작동하는 경사하강법으로, 전체 훈련 데이터 세트를 여러 작은 그룹(Mini Batch)들로 나눠 경사하강법을 진행. 하나의 반복에서 하나의 미니 배치에 속해 있는 데이터들에 대하여 기울기를 구한 후, 그것의 평균 기울기를 통해 매개변수를 업데이트하는 방식.

  - 장점:
    - 빠른 학습 제공
    - Local optimal에 갇힌 경우 노이즈를 통해 벗어날 수 있다.
    - 벡터화의 이점을 누릴 수 있다.
  - 크기 설정
    - 훈련 데이터 세트가 작다면 (m < 200) 그냥 배치 경사하강법을 사용하자.
    - 일반적으로 Batch size는 2의 n제곱 (64, 128, 256, 512 등)으로 설정한다. (컴퓨터는 2진수로 계산이 되기 때문)

## 오늘의 회고
  - 뭔가 근래에 해야 할 게 많은 것 같다는 느낌이 들어 심리적 부담감이 늘었다.
  - 생각만 하고 하는게 없기에 천천히 오래 진행을 해서 하루에 뭐라도 하나를 해야겠다는 생각이 들어 오늘부터는 시간이 날 때마다 공부와 코드 작성을 진행해야겠다.